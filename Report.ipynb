{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marine-trick",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-groove",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "- Two deep learning networks (Actor and Critic Networks)\n",
    "- Specified in `model.py`\n",
    "- Three fully connected (FC) layers \n",
    "  - Actor Network: All Relu Activations except the output which is tanh\n",
    "  - Actor Network: Input (33 nodes) > FC (128 nodes) > Relu > FC (64 nodes) > Relu > FC (4 nodes) > tanh > Output\n",
    "  - Critic Network: Input (33 nodes) > FC (128 nodes) > Relu > FC (68 nodes) > Relu > FC (1 node) > Relu > Output\n",
    "- Optimization algorithm is ADAM optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-reader",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "- RL Discount Factor (GAMMA) = 0.99\n",
    "- Soft update parameter (TAU) = 1e-3\n",
    "- LEARNING_RATE_ACTOR  = 1e-3\n",
    "- LEARNING_RATE_CRITIC  = 1e-3\n",
    "- Memory Replay Buffer (BUFFER_SIZE) = 1e5\n",
    "- Mini-batch (BATCH_SIZE) = 128\n",
    "- WEIGHT_DECAY = 0\n",
    "\n",
    "### Reinforcement Learning Algorithm\n",
    "\n",
    "- The Algorithm is Deep Deterministic Policy Gradient  (DDPG)\n",
    "\n",
    "- It relies on two deep learning networks (actor network and critic network)\n",
    "\n",
    "- The critic network in this algorithm is like the dqn network but can handle continuous action values as it receives the output of the actor network which represents the chosen action.\n",
    "\n",
    "- Whereas the actor network is trained to output the best action `μ(s; θ)` for a given state. Unlike typical actor-critic algorithm in which the actor outputs the policy `π(a | s; θ)`, where\n",
    "  - `s`: state\n",
    "  - `θ`: model parameters\n",
    "  - `a`: action\n",
    "\n",
    "- The Neural Network model is softly updated to a target model in order to keep the learning stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-powder",
   "metadata": {},
   "source": [
    "## Plot of Rewards (score)\n",
    "\n",
    "---\n",
    "\n",
    "![scores plot](scores-plot.png)\n",
    "### Scores history\n",
    "- As shown in the following figure the agent started to reach the set goal on the 184<sup>th</sup> episode\n",
    "- The average scores remains over 30\n",
    "\n",
    "\n",
    "![scores history](scores-log.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-cartoon",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "---\n",
    "\n",
    "- Actor Critic seems to be working perfectly with this problem, but for a future work I am speculating that PPO might show better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
